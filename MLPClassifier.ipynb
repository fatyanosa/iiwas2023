{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "1. Read the dataset.\n",
    "2. Reshape the dataset\n",
    "2. Split the dataset to:\n",
    "    * 70% training (only contain normal data),\n",
    "    * 10% validation (contain normal and anomaly data), and\n",
    "    * 20% testing dataset (contain normal and anomaly data)\n",
    "3. Create autoencoder model.\n",
    "4. Train the model using training dataset.\n",
    "5. Validate the model using validation dataset.\n",
    "6. Test the model using testing dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 25\n",
    "value_range = (0, 40000)\n",
    "\n",
    "test_size = 0.25\n",
    "random_state = 11\n",
    "\n",
    "# Define model's hyperparameters\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_epochs = 4000\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('/nas.dbms/mahendra.data/Documents/annotation/NAB/data/realKnownCause/nyc_taxi.csv',low_memory=False)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "\n",
    "# The times of anomaly events (Ground Truth)\n",
    "anomaly_points = [\n",
    "        [\n",
    "            \"2014-11-01 06:00:00.000000\",\n",
    "            \"2014-11-02 06:00:00.000000\"\n",
    "        ],\n",
    "        [\n",
    "            \"2014-11-27 06:00:00.000000\",\n",
    "            \"2014-11-28 06:00:00.000000\"\n",
    "        ],\n",
    "        [\n",
    "            \"2014-12-25 06:00:00.000000\",\n",
    "            \"2014-12-26 06:00:00.000000\"\n",
    "        ],\n",
    "        [\n",
    "            \"2014-12-31 06:00:00.000000\",\n",
    "            \"2015-01-01 06:00:00.000000\"\n",
    "        ],\n",
    "        [\n",
    "            \"2015-01-26 16:00:00.000000\",\n",
    "            \"2015-01-28 05:00:00.000000\"\n",
    "        ]\n",
    "]\n",
    "\n",
    "# Normal label: 0, Anomaly label: 1\n",
    "df['anomaly'] = 0  # Set default values\n",
    "for start, end in anomaly_points:\n",
    "    df.loc[((df['timestamp'] >= start) & (df['timestamp'] <= end)), 'anomaly'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(\"\\nDataset size:\")\n",
    "print(df.shape)\n",
    "print(\"\\nDataset distribution:\")\n",
    "print(df['anomaly'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomaly(df, label='anomaly', plot_item='value', time_col='timestamp', scatter=True):\n",
    "    # Plot function\n",
    "    plotter =  plt.scatter if scatter else plt.plot\n",
    "\n",
    "    # Plot sections\n",
    "    plt.close()\n",
    "    plt.figure().set_figwidth(20)\n",
    "\n",
    "    color = {0: 'blue', 1: 'red'}\n",
    "    for i, c in color.items():\n",
    "        data = df[df[label] == i].copy(deep=True)  # Select data\n",
    "        data.index = data[time_col]  # Change index\n",
    "        plotter(data.index, data[plot_item], color=c)  # Plot\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_cm(cm):\n",
    "    plt.close()\n",
    "    # plt.figure(figsize=(8,8))\n",
    "    # sns.set(font_scale = 1.5)\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        cm, # confusion matrix 2D array \n",
    "        annot=True, # show numbers in the cells\n",
    "        fmt='d', # show numbers as integers\n",
    "        xticklabels=[0,1],\n",
    "        yticklabels=[0,1]\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 8700\n",
    "df_train = df[:cut]  # Select three sections of anomaly for training\n",
    "plot_anomaly(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[8700:]  # Select two sections of anomaly for testing\n",
    "plot_anomaly(df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, window_size, value_range):\n",
    "    X, y = [], []\n",
    "    for i in range(df.shape[0]-window_size):\n",
    "        feature = df['value'].iloc[i:i+window_size]\n",
    "        target = df['anomaly'].iloc[i:i+window_size]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "\n",
    "    # Convert to numpy\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Normalization\n",
    "    X = (X - value_range[0]) / (value_range[1] - value_range[0])\n",
    "\n",
    "    # Calculate the median of the labels\n",
    "    y = np.round(np.mean(y, axis=1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Create training and testing dataset\n",
    "X_train, y_train = create_dataset(df_train, window_size, value_range)\n",
    "X_test, y_test = create_dataset(df_test, window_size, value_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training dataset\")\n",
    "print(\"X shape:\", X_train.shape)\n",
    "print(\"y shape:\", y_train.shape)\n",
    "print(\"Anomaly distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "print(\"\\nTesting dataset\")\n",
    "print(\"X shape:\", X_test.shape)\n",
    "print(\"y shape:\", y_test.shape)\n",
    "print(\"Anomaly distribution:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define the MLP model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fcOut = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fcOut(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def predict(model, features_tensor):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_outputs = model(features_tensor)\n",
    "        return torch.round(torch.sigmoid(train_outputs)).squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define input size\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Create the MLP model\n",
    "model = MLPClassifier(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "progress_bar = tqdm(range(num_epochs),ncols=150)\n",
    "progress_bar.set_description(\"Training\")\n",
    "for epoch in progress_bar:\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for batch_features, batch_labels in dataloader:\n",
    "        # Move batch to GPU\n",
    "        batch_features = batch_features.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs.squeeze(), batch_labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss after each epoch\n",
    "    progress_bar.set_postfix(Loss=\"%.4f\" % loss.item())\n",
    "\n",
    "\n",
    "# Train prediction\n",
    "train_predictions = predict(model, X_train_tensor)\n",
    "\n",
    "# Test prediction\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "test_predictions = predict(model, X_test_tensor)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_matrix(y, y_pred):\n",
    "    ps = precision_score(y, y_pred, pos_label=1, labels=[0, 1])\n",
    "    print(\"precision_score: %.2f\" % ps)\n",
    "\n",
    "    rs = recall_score(y, y_pred, pos_label=1, labels=[0, 1])\n",
    "    print(\"recall_score: %.2f\" % rs)\n",
    "\n",
    "    f1 = f1_score(y, y_pred, pos_label=1, labels=[0, 1])\n",
    "    print(\"f1_score: %.2f\" % f1)\n",
    "\n",
    "    cm = confusion_matrix(y, y_pred, labels=[0, 1])\n",
    "    plot_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_matrix(y_train, train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_matrix(y_test, test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mida",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
